{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf sdk_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (1.11.0+cpu)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (0.12.0+cpu)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.9/site-packages (0.11.0+cpu)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch) (4.0.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset only Download one of the data sets\n",
    "#!if [[ -d final_humor_sdk ]]; then rm -rf final_humor_sdk; fi\n",
    "#!curl -L -o original_data.zip https://www.dropbox.com/s/izk6khkrdwcncia/ted_humor_sdk_v1.zip\\?dl\\=1\n",
    "#!unzip original_data.zip\n",
    "#!rm original_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   143    0   143    0     0    467      0 --:--:-- --:--:-- --:--:--   467\n",
      "100   363  100   363    0     0    429      0 --:--:-- --:--:-- --:--:--     0\n",
      " 64 17.7G   64 11.3G    0     0  11.6M      0  0:26:05  0:16:45  0:09:20 10.9M:00:15  0:29:48 11.0M0M0  11.5M      0  0:26:11  0:10:30  0:15:41 10.1M:11:04  0:15:04 12.2M 0:26:02  0:12:03  0:13:59 11.5M 0:12:31  0:13:33 11.2M     0  11.6M      0  0:26:04  0:16:21  0:09:43 12.0M^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# V2 Raw Videos only Download one of the data sets\n",
    "!if [[ -d final_humor_sdk ]]; then rm -rf final_humor_sdk; fi\n",
    "!curl -L -o v2_raw_data.zip https://www.dropbox.com/s/lg7kjx0kul3ansq/urfunny2_videos.zip?dl=1\n",
    "!unzip v2_raw_data.zip\n",
    "!rm v2_raw_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 Extracted Features only Download one of the data sets\n",
    "!if [[ -d final_humor_sdk ]]; then rm -rf final_humor_sdk; fi\n",
    "!curl -L -o https://www.dropbox.com/sh/9h0pcqmqoplx9p2/AAC8yYikSBVYCSFjm3afFHQva?dl=1\n",
    "!unzip v2_extracted_data.zip\n",
    "!rm v2_extracted_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_pickle(pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "    except UnicodeDecodeError as e:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f, encoding='latin1')\n",
    "    except Exception as e:\n",
    "        print('Unable to load data ', pickle_file, ':', e)\n",
    "        raise\n",
    "    return pickle_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "you can assign the maximum number number of sentences in context and what will be the maximum number of words of any sentence.\n",
    "\n",
    "It will do left padding . It will concatenate the word embedding + covarep features + openface features\n",
    "\n",
    "example:\n",
    "\n",
    "if max_sen_len = 20 then the punchline sentence dimension = 20 * 752. \n",
    "    where 752 = word embedding (300) + covarep (81) + openface(371)  \n",
    "\n",
    "if max_sen_len = 20 and max_context_len = 5 that means context can have maximum 5 sentences \n",
    "and each sentence will have maximum 20 words. The context dimension will be 5 * 20 * 752 \n",
    "\n",
    "We will do left padding with zeros to maintaing the same dimension.\n",
    "\n",
    "In our experiments we set max_sen_len = 20 & max_context_len = 5 \n",
    "'''\n",
    "\n",
    "\n",
    "class HumorDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, id_list,max_context_len=5,max_sen_len=20):\n",
    "        self.id_list = id_list\n",
    "        openface_file=\"sdk_features/openface_features_sdk.pkl\"\n",
    "        covarep_file=\"sdk_features/covarep_features_sdk.pkl\"\n",
    "        language_file=\"sdk_features/language_sdk.pkl\"\n",
    "        word_embedding_list_file=\"sdk_features/word_embedding_list.pkl\"\n",
    "        humor_label_file=\"sdk_features/humor_label_sdk.pkl\"\n",
    "        \n",
    "        self.word_aligned_openface_sdk=load_pickle(openface_file)\n",
    "        self.word_aligned_covarep_sdk=load_pickle(covarep_file)\n",
    "        self.language_sdk=load_pickle(language_file)\n",
    "        self.word_embedding_list_sdk=load_pickle(word_embedding_list_file)\n",
    "        self.humor_label_sdk = load_pickle(humor_label_file)\n",
    "        self.of_d=371\n",
    "        self.cvp_d=81\n",
    "        self.glove_d=300\n",
    "        self.total_dim=self.glove_d+self.of_d+self.cvp_d\n",
    "        self.max_context_len=max_context_len\n",
    "        self.max_sen_len=max_sen_len\n",
    "    \n",
    "    #left padding with zero  vector upto maximum number of words in a sentence * glove embedding dimension \n",
    "    def paded_word_idx(self,seq,max_sen_len=20,left_pad=1):\n",
    "        seq=seq[0:max_sen_len]\n",
    "        pad_w=np.concatenate((np.zeros(max_sen_len-len(seq)),seq),axis=0)\n",
    "        pad_w=np.array([self.word_embedding_list_sdk[int(w_id)] for  w_id in pad_w])\n",
    "        return pad_w\n",
    "    \n",
    "    #left padding with zero  vector upto maximum number of words in a sentence * covarep dimension \n",
    "    def padded_covarep_features(self,seq,max_sen_len=20,left_pad=1):\n",
    "        seq=seq[0:max_sen_len]\n",
    "        return np.concatenate((np.zeros((max_sen_len-len(seq),self.cvp_d)),seq),axis=0)\n",
    "    \n",
    "    #left padding with zero  vector upto maximum number of words in a sentence * openface dimension \n",
    "    def padded_openface_features(self,seq,max_sen_len=20,left_pad=1):\n",
    "        seq=seq[0:max_sen_len]\n",
    "        return np.concatenate((np.zeros(((max_sen_len-len(seq)),self.of_d)),seq),axis=0)\n",
    "    \n",
    "    #left padding with zero vectors upto maximum number of sentences in context * maximum num of words in a sentence * 456\n",
    "    def padded_context_features(self,context_w,context_of,context_cvp,max_context_len=5,max_sen_len=20):\n",
    "        context_w=context_w[-max_context_len:]\n",
    "        context_of=context_of[-max_context_len:]\n",
    "        context_cvp=context_cvp[-max_context_len:]\n",
    "\n",
    "        padded_context=[]\n",
    "        for i in range(len(context_w)):\n",
    "            p_seq_w=self.paded_word_idx(context_w[i],max_sen_len)\n",
    "            p_seq_cvp=self.padded_covarep_features(context_cvp[i],max_sen_len)\n",
    "            p_seq_of=self. padded_openface_features(context_of[i],max_sen_len)\n",
    "            padded_context.append(np.concatenate((p_seq_w,p_seq_cvp,p_seq_of),axis=1))\n",
    "\n",
    "        pad_c_len=max_context_len-len(padded_context)\n",
    "        padded_context=np.array(padded_context)\n",
    "        \n",
    "        #if there is no context\n",
    "        if not padded_context.any():\n",
    "            return np.zeros((max_context_len,max_sen_len,self.total_dim))\n",
    "        \n",
    "        return np.concatenate((np.zeros((pad_c_len,max_sen_len,self.total_dim)),padded_context),axis=0)\n",
    "    \n",
    "    def padded_punchline_features(self,punchline_w,punchline_of,punchline_cvp,max_sen_len=20,left_pad=1):\n",
    "        \n",
    "        p_seq_w=self.paded_word_idx(punchline_w,max_sen_len)\n",
    "        p_seq_cvp=self.padded_covarep_features(punchline_cvp,max_sen_len)\n",
    "        p_seq_of=self.padded_openface_features(punchline_of,max_sen_len)\n",
    "        return np.concatenate((p_seq_w,p_seq_cvp,p_seq_of),axis=1)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id_list)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        hid=self.id_list[index]\n",
    "        punchline_w=np.array(self.language_sdk[hid]['punchline_embedding_indexes'])\n",
    "        punchline_of=np.array(self.word_aligned_openface_sdk[hid]['punchline_features'])\n",
    "        punchline_cvp=np.array(self.word_aligned_covarep_sdk[hid]['punchline_features'])\n",
    "        \n",
    "        context_w=np.array(self.language_sdk[hid]['context_embedding_indexes'])\n",
    "        context_of=np.array(self.word_aligned_openface_sdk[hid]['context_features'])\n",
    "        context_cvp=np.array(self.word_aligned_covarep_sdk[hid]['context_features'])\n",
    "        \n",
    "        #punchline feature\n",
    "        x_p=torch.LongTensor(self.padded_punchline_features(punchline_w,punchline_of,punchline_cvp,self.max_sen_len))\n",
    "        #context feature\n",
    "        x_c=torch.LongTensor(self.padded_context_features(context_w,context_of,context_cvp,self.max_context_len,self.max_sen_len))\n",
    "        \n",
    "        y=torch.FloatTensor([self.humor_label_sdk[hid]])\n",
    "                \n",
    "        return x_p,x_c,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folds_file=\"sdk_features/data_folds.pkl\"\n",
    "data_folds=load_pickle(data_folds_file)\n",
    "train=data_folds['train']\n",
    "dev=data_folds['dev']\n",
    "test=data_folds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9588"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)+len(dev)+len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HumorDataset(train)\n",
    "dev_set = HumorDataset(dev)\n",
    "test_set = HumorDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=10\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_set, batch_size=batch, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_p.shape=batch_size*maximum number of words in sentence * 752\n",
    "x_c.shape = batch_size * maximum context length in #sentences * maximum sentence length in #words * 752\n",
    "here 752 = word embedding (300) + covarep (81) + openface(371)  \n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for  batch_idx, batch in enumerate(train_dataloader, 0): \n",
    "    x_p,x_c,y=map(lambda x: x.to(device), batch)\n",
    "    print(\"*********\")\n",
    "    print(\"punchline shape: \",x_p.shape)\n",
    "    print(\"context shape: \",x_c.shape)\n",
    "    print(\"humor labels: \",y)\n",
    "    if batch_idx==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
